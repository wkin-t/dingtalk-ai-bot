# -*- coding: utf-8 -*-
"""
Gemini å®˜æ–¹ SDK å®¢æˆ·ç«¯ (ä½¿ç”¨æ–°ç‰ˆ google-genai)
æ”¯æŒæå–çœŸå®çš„ thinking å†…å®¹
æ”¯æŒ Google Search Grounding (å®æ—¶æœç´¢)
"""
import os
import time
import asyncio
from typing import List, Dict, Any, Optional, AsyncGenerator
from google import genai
from google.genai import types
from app.config import GEMINI_API_KEY, DEFAULT_MODEL, ENABLE_THINKING, SOCKS_PROXY, ENABLE_SEARCH

# é…ç½®ä»£ç† (ä»… Gemini API ä½¿ç”¨ä»£ç†ï¼Œé€šè¿‡ httpx_client å•ç‹¬é…ç½®)
# å°† socks5h:// è½¬æ¢ä¸º socks5:// (httpx æ ¼å¼)
proxy_url = SOCKS_PROXY.replace("socks5h://", "socks5://") if SOCKS_PROXY else None

# åˆ›å»ºå¸¦ä»£ç†çš„ httpx Client (ä»…ç”¨äº Gemini SDK)
if proxy_url:
    import httpx
    print(f"ğŸ”— Gemini SDK ä½¿ç”¨ä»£ç†: {proxy_url}")
    _httpx_client = httpx.Client(proxy=proxy_url, timeout=60.0)
    client = genai.Client(
        api_key=GEMINI_API_KEY,
        http_options=types.HttpOptions(
            api_version='v1beta',
            httpx_client=_httpx_client
        )
    )
else:
    print("ğŸ”— Gemini SDK ç›´è¿ (æ— ä»£ç†)")
    client = genai.Client(
        api_key=GEMINI_API_KEY,
        http_options=types.HttpOptions(api_version='v1beta')
    )


async def analyze_complexity_with_model(content: str, has_images: bool = False) -> dict:
    """
    ä½¿ç”¨ Gemini Flash Lite å¿«é€Ÿåˆ†æé—®é¢˜å¤æ‚åº¦
    è¿”å›æ¨èçš„æ¨¡å‹ã€thinking level å’Œæ˜¯å¦éœ€è¦è”ç½‘æœç´¢

    Returns:
        {
            "model": "gemini-3-flash-preview" or "gemini-3-pro-preview",
            "thinking_level": "minimal" | "low" | "medium" | "high",
            "need_search": true | false,
            "reason": "åˆ†æåŸå› "
        }
    """
    import asyncio
    import json
    import re
    import traceback

    print(f"ğŸ” [é¢„åˆ†æ] å‡½æ•°è¢«è°ƒç”¨ï¼Œå†…å®¹: {content[:50]}...")
    print(f"ğŸ” [é¢„åˆ†æ] has_images={has_images}")

    # æ„é€ åˆ†ææç¤º
    analysis_prompt = f"""åˆ†æç”¨æˆ·é—®é¢˜ï¼Œè¿”å› JSON è·¯ç”±å»ºè®®ã€‚

é—®é¢˜: {content[:300]}
æœ‰å›¾ç‰‡: {"æ˜¯" if has_images else "å¦"}

é€‰æ‹©è§„åˆ™:
1. model:
   - "gemini-3-flash-preview": æ—¥å¸¸é—®ç­”ã€ä»£ç ã€ä¸€èˆ¬åˆ†æ (é»˜è®¤)
   - "gemini-3-pro-preview": ä»…ç”¨äºå¤æ‚æ•°å­¦è¯æ˜ã€å­¦æœ¯ç ”ç©¶ã€ç³»ç»Ÿæ¶æ„è®¾è®¡

2. thinking_level:
   - "minimal": ç®€å•é—®å€™å¦‚"ä½ å¥½"ã€"è°¢è°¢"
   - "low": æ™®é€šé—®ç­”ã€äº‹å®æŸ¥è¯¢
   - "medium": éœ€è¦ä¸€å®šæ¨ç†ã€ä»£ç é—®é¢˜
   - "high": å¤æ‚åˆ†æã€ç®—æ³•è®¾è®¡

3. need_search:
   - true: éœ€è¦å®æ—¶ä¿¡æ¯ï¼ˆå¤©æ°”ã€æ–°é—»ã€è‚¡ä»·ã€æœ€æ–°äº‹ä»¶ï¼‰
   - false: ä¸éœ€è¦è”ç½‘ï¼ˆé»˜è®¤ï¼‰

åªè¿”å›JSON:
{{"model":"gemini-3-flash-preview","thinking_level":"low","need_search":false,"reason":"ç®€çŸ­åŸå› "}}"""

    try:
        print(f"ğŸ” [é¢„åˆ†æ] å‡†å¤‡è°ƒç”¨ gemini-flash-lite-latest...")
        loop = asyncio.get_running_loop()

        def _analyze():
            print(f"ğŸ” [é¢„åˆ†æ] è¿›å…¥çº¿ç¨‹æ‰§è¡Œå™¨...")
            response = client.models.generate_content(
                model="gemini-flash-lite-latest",
                contents=[types.Content(role="user", parts=[types.Part.from_text(text=analysis_prompt)])],
                config=types.GenerateContentConfig(
                    temperature=0.1,
                    max_output_tokens=150
                )
            )
            print(f"ğŸ” [é¢„åˆ†æ] API è°ƒç”¨å®Œæˆ")
            return response.text

        result_text = await loop.run_in_executor(None, _analyze)
        print(f"ğŸ“ [é¢„åˆ†æ] åŸå§‹è¿”å›: {result_text[:200]}")

        # è§£æ JSON
        json_match = re.search(r'\{[^}]+\}', result_text)
        if json_match:
            result = json.loads(json_match.group())
            # éªŒè¯å’Œä¿®æ­£å­—æ®µ
            if result.get("model") not in ["gemini-3-flash-preview", "gemini-3-pro-preview"]:
                result["model"] = "gemini-3-flash-preview"
            if result.get("thinking_level") not in ["minimal", "low", "medium", "high"]:
                result["thinking_level"] = "low"
            if "need_search" not in result:
                result["need_search"] = False
            print(f"ğŸ¤– é¢„åˆ†æç»“æœ: {result}")
            return result
        else:
            print(f"âš ï¸ æ— æ³•ä»è¿”å›ä¸­æå– JSON: {result_text}")

    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹é¢„åˆ†æå¤±è´¥: {e}")
        traceback.print_exc()

    # é™çº§ï¼šè¿”å›ä¿å®ˆçš„é»˜è®¤å€¼
    print("âš ï¸ ä½¿ç”¨é™çº§é»˜è®¤é…ç½®")
    return {
        "model": "gemini-3-flash-preview",
        "thinking_level": "low",
        "need_search": False,
        "reason": "é¢„åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®"
    }


def _convert_openai_to_gemini(messages: List[Dict[str, Any]]) -> tuple[Optional[str], List[types.Content]]:
    """
    å°† OpenAI æ ¼å¼çš„æ¶ˆæ¯è½¬æ¢ä¸º Gemini æ ¼å¼

    OpenAI æ ¼å¼:
    [
        {"role": "system", "content": "..."},
        {"role": "user", "content": "..."},
        {"role": "assistant", "content": "..."}
    ]

    Gemini æ ¼å¼:
    system_instruction: "..."
    contents: [Content(role="user", parts=[...]), Content(role="model", parts=[...])]
    """
    system_instruction = None
    contents = []

    for msg in messages:
        role = msg.get("role", "user")
        content = msg.get("content", "")

        # æå– system prompt
        if role == "system":
            system_instruction = content
            continue

        # è½¬æ¢è§’è‰²åç§°: assistant -> model
        gemini_role = "model" if role == "assistant" else "user"

        # å¤„ç†å†…å®¹ (å¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å¤šæ¨¡æ€åˆ—è¡¨)
        parts = []
        if isinstance(content, str):
            parts.append(types.Part.from_text(text=content))
        elif isinstance(content, list):
            # å¤šæ¨¡æ€å†…å®¹ (æ–‡æœ¬ + å›¾ç‰‡)
            for item in content:
                if item.get("type") == "text":
                    parts.append(types.Part.from_text(text=item.get("text", "")))
                elif item.get("type") == "image_url":
                    # ä» data URL æå– base64
                    image_url = item.get("image_url", {}).get("url", "")
                    if image_url.startswith("data:"):
                        try:
                            header, b64_data = image_url.split(",", 1)
                            mime_type = header.split(":")[1].split(";")[0]
                            import base64
                            image_bytes = base64.b64decode(b64_data)
                            parts.append(types.Part.from_bytes(
                                data=image_bytes,
                                mime_type=mime_type
                            ))
                        except Exception as e:
                            print(f"âš ï¸ è§£æå›¾ç‰‡ data URL å¤±è´¥: {e}")
        else:
            parts.append(types.Part.from_text(text=str(content)))

        contents.append(types.Content(role=gemini_role, parts=parts))

    return system_instruction, contents


async def call_gemini_stream(
    messages: List[Dict[str, Any]],
    target_model: str = DEFAULT_MODEL,
    thinking_level: str = "low",
    enable_search: bool = False  # ç”±æ™ºèƒ½è·¯ç”±å†³å®šæ˜¯å¦å¯ç”¨æœç´¢
) -> AsyncGenerator[Dict[str, str], None]:
    """
    è°ƒç”¨ Gemini API è¿›è¡Œæµå¼ç”Ÿæˆ

    Args:
        messages: OpenAI æ ¼å¼çš„æ¶ˆæ¯åˆ—è¡¨
        target_model: æ¨¡å‹åç§°
        thinking_level: æ€è€ƒæ·±åº¦
        enable_search: æ˜¯å¦å¯ç”¨ Google Search

    Yields:
        {"content": "...", "thinking": "..."} æˆ– {"error": "..."}
    """
    print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚ Google Gemini API ({target_model})...")

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    input_tokens = 0
    output_tokens = 0

    try:
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        system_instruction, contents = _convert_openai_to_gemini(messages)

        # é…ç½®å·¥å…· (Google Search ç”±æ™ºèƒ½è·¯ç”±å†³å®š)
        tools = []
        if enable_search:
            tools.append(types.Tool(google_search=types.GoogleSearch()))
            print("ğŸ” å·²å¯ç”¨ Google Search (å®æ—¶æœç´¢)")

        # é…ç½®ç”Ÿæˆå‚æ•°
        config = types.GenerateContentConfig(
            temperature=0.7,
            max_output_tokens=8192,
            system_instruction=system_instruction,
            tools=tools if tools else None,
            safety_settings=[
                types.SafetySetting(
                    category="HARM_CATEGORY_HARASSMENT",
                    threshold="BLOCK_NONE"
                ),
                types.SafetySetting(
                    category="HARM_CATEGORY_HATE_SPEECH",
                    threshold="BLOCK_NONE"
                ),
                types.SafetySetting(
                    category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    threshold="BLOCK_NONE"
                ),
                types.SafetySetting(
                    category="HARM_CATEGORY_DANGEROUS_CONTENT",
                    threshold="BLOCK_NONE"
                ),
            ]
        )

        # Gemini 3 ç³»åˆ—æ”¯æŒ thinkingï¼Œé…ç½® thinking level
        # thinking_level: minimal (æœ€å¿«) | low | medium | high (æœ€æ·±åº¦)
        if 'gemini-3' in target_model.lower() or 'thinking' in target_model.lower():
            if ENABLE_THINKING and thinking_level != "minimal":
                config.thinking_config = types.ThinkingConfig(
                    thinking_level=thinking_level,
                    include_thoughts=True
                )
                print(f"ğŸ§  å·²å¯ç”¨ Thinking æ¨¡å¼ (level={thinking_level})")
            else:
                config.thinking_config = types.ThinkingConfig(
                    thinking_level="minimal",
                    include_thoughts=False
                )
                print("âš¡ Thinking æ¨¡å¼ (level=minimal, æœ€å¿«å“åº”)")
                print("âš¡ Thinking æ¨¡å¼ (level=low, å¿«é€Ÿå“åº”)")

        # åŒæ­¥æµå¼ç”Ÿæˆ (åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ)
        def _stream_generate():
            return client.models.generate_content_stream(
                model=target_model,
                contents=contents,
                config=config
            )

        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, _stream_generate)

        # æ ‡è®°æ˜¯å¦å·²å‘é€ thinking å†…å®¹
        thinking_sent = False

        # è¿­ä»£æµå¼å“åº”
        for chunk in response:
            try:
                # æå– usage_metadata (token ç»Ÿè®¡)
                if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:
                    usage = chunk.usage_metadata
                    if hasattr(usage, 'prompt_token_count'):
                        input_tokens = usage.prompt_token_count or 0
                    if hasattr(usage, 'candidates_token_count'):
                        output_tokens = usage.candidates_token_count or 0

                # æ£€æŸ¥æ˜¯å¦æœ‰å€™é€‰å†…å®¹
                if not chunk.candidates:
                    continue

                candidate = chunk.candidates[0]
                if not candidate.content or not candidate.content.parts:
                    continue

                for part in candidate.content.parts:
                    # part.thought æ˜¯å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºè¿™ä¸ª part æ˜¯å¦æ˜¯æ€è€ƒå†…å®¹
                    # æ€è€ƒå†…å®¹å’Œæ­£å¼å›å¤éƒ½åœ¨ part.text é‡Œ
                    is_thought = getattr(part, 'thought', False)
                    text_content = getattr(part, 'text', '')

                    if not text_content:
                        continue

                    if is_thought:
                        # è¿™æ˜¯æ€è€ƒå†…å®¹
                        if ENABLE_THINKING and not thinking_sent:
                            yield {"thinking_start": True}
                            thinking_sent = True
                        yield {"thinking": text_content}
                    else:
                        # è¿™æ˜¯æ­£å¼å›å¤
                        if thinking_sent:
                            yield {"thinking_end": True}
                            thinking_sent = False
                        yield {"content": text_content}

            except ValueError as e:
                print(f"âš ï¸ Chunk å¤„ç†è­¦å‘Š: {e}")
                continue
            except Exception as e:
                print(f"âš ï¸ å¤„ç† chunk å¼‚å¸¸: {e}")
                continue

        # è®¡ç®—å»¶è¿Ÿ
        latency_ms = int((time.time() - start_time) * 1000)
        print(f"âœ… æµå¼å“åº”ç»“æŸ | è¾“å…¥: {input_tokens} tokens, è¾“å‡º: {output_tokens} tokens, å»¶è¿Ÿ: {latency_ms}ms")

        # è¿”å›ç»Ÿè®¡ä¿¡æ¯
        yield {
            "usage": {
                "model": target_model,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "latency_ms": latency_ms
            }
        }

    except Exception as e:
        error_msg = str(e)
        print(f"âŒ Gemini API é”™è¯¯: {error_msg}")
        yield {"error": f"Gemini API Error: {error_msg}"}
