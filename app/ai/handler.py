# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€ AI å¤„ç†å±‚ - æŠ½è±¡å¹³å°å·®å¼‚
"""
import asyncio
import time
from typing import Optional, Dict, List, Callable
from datetime import datetime, timezone, timedelta
from app.config import (
    MAX_HISTORY_LENGTH, DEFAULT_MODEL, AI_BACKEND, BOT_ID, OPENCLAW_CONTEXT_MESSAGES,
    get_model_pricing
)
from app.memory import get_history, update_history
from app.gemini_client import call_gemini_stream, analyze_complexity_with_model
from app.ai.router import analyze_complexity_unified


class AIHandler:
    """
    ç»Ÿä¸€ AI å¤„ç†å™¨ - æŠ½è±¡å¹³å°å·®å¼‚

    æ”¯æŒ:
    - é’‰é’‰ (æµå¼å¡ç‰‡æ›´æ–°)
    - ä¼ä¸šå¾®ä¿¡ (å®Œæ•´å›å¤)
    """

    def __init__(self, platform: str = "dingtalk"):
        """
        åˆå§‹åŒ– AI å¤„ç†å™¨

        Args:
            platform: å¹³å°ç±»å‹ (dingtalk | wecom)
        """
        self.platform = platform

    async def process_message(
        self,
        content: str,
        session_key: str,
        user_id: str,
        sender_nick: str = "User",
        image_data_list: Optional[List[bytes]] = None,
        group_info: Optional[Dict] = None,
        stream_callback: Optional[Callable] = None,
        complete_callback: Optional[Callable] = None
    ) -> str:
        """
        å¤„ç†æ¶ˆæ¯å¹¶è°ƒç”¨ AI

        Args:
            content: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            session_key: ä¼šè¯é”®
            user_id: ç”¨æˆ· ID
            sender_nick: å‘é€è€…æ˜µç§°
            image_data_list: å›¾ç‰‡æ•°æ®åˆ—è¡¨ (å¯é€‰)
            group_info: ç¾¤ä¿¡æ¯ (å¯é€‰)
            stream_callback: æµå¼æ›´æ–°å›è°ƒ (thinking, content) -> None
            complete_callback: å®Œæˆå›è°ƒ (response) -> None

        Returns:
            AI å®Œæ•´å›å¤
        """
        print(f"ğŸš€ [AIHandler] å¼€å§‹å¤„ç†æ¶ˆæ¯: {content} (User: {user_id}, Platform: {self.platform})")

        # è·å–å®Œæ•´å†å²è®°å½•
        full_history = get_history(session_key)

        # OpenClaw æ¨¡å¼ä½¿ç”¨è½»é‡ä¸Šä¸‹æ–‡ï¼Œé¿å…è¦†ç›– Gateway ä¾§ agent/system ç­–ç•¥
        if AI_BACKEND == "openclaw":
            if OPENCLAW_CONTEXT_MESSAGES > 0 and len(full_history) > OPENCLAW_CONTEXT_MESSAGES:
                history_messages = full_history[-OPENCLAW_CONTEXT_MESSAGES:]
            else:
                history_messages = full_history if OPENCLAW_CONTEXT_MESSAGES > 0 else []

            messages = []
            for msg in history_messages:
                role = msg.get("role")
                msg_content = msg.get("content", "")
                if role in {"user", "assistant"} and msg_content:
                    messages.append({"role": role, "content": msg_content})

            if image_data_list:
                import base64
                user_message_content = [{
                    "type": "text",
                    "text": f"{sender_nick}: [å›¾ç‰‡x{len(image_data_list)}] {content}"
                }]
                for i, img_data in enumerate(image_data_list):
                    b64_image = base64.b64encode(img_data).decode('utf-8')
                    print(f"ğŸ–¼ï¸ å¤„ç†ç¬¬ {i+1} å¼ å›¾ç‰‡ï¼Œå¤§å°: {len(img_data)} bytes")
                    user_message_content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{b64_image}"}
                    })
                messages.append({"role": "user", "content": user_message_content})
            else:
                messages.append({"role": "user", "content": f"{sender_nick}: {content}"})
        else:
            # æˆªå–æœ€è¿‘çš„ N æ¡å‘é€ç»™ AI
            if len(full_history) > MAX_HISTORY_LENGTH:
                history_messages = full_history[-MAX_HISTORY_LENGTH:]
            else:
                history_messages = full_history

            # æ„é€  System Prompt
            system_prompt = self._build_system_prompt(group_info)

            messages = [{"role": "system", "content": system_prompt}]

            # æ ¼å¼åŒ–å†å²æ¶ˆæ¯
            formatted_history = self._format_history(history_messages)

            # æ„é€ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            beijing_tz = timezone(timedelta(hours=8))
            current_timestamp = datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S")

            if image_data_list:
                # å¤šæ¨¡æ€æ¶ˆæ¯
                import base64
                user_message_content = []
                user_message_content.append({
                    "type": "text",
                    "text": f"[{current_timestamp}] {sender_nick}: [å›¾ç‰‡x{len(image_data_list)}] {content}"
                })

                for i, img_data in enumerate(image_data_list):
                    b64_image = base64.b64encode(img_data).decode('utf-8')
                    print(f"ğŸ–¼ï¸ å¤„ç†ç¬¬ {i+1} å¼ å›¾ç‰‡ï¼Œå¤§å°: {len(img_data)} bytes")
                    user_message_content.append({
                        "type": "image_url",
                        "image_url": {"url": f"data:image/jpeg;base64,{b64_image}"}
                    })

                messages.extend(formatted_history)
                messages.append({"role": "user", "content": user_message_content})
            else:
                # çº¯æ–‡æœ¬æ¶ˆæ¯
                text_content = f"[{current_timestamp}] {sender_nick}: {content}"
                messages.extend(formatted_history)
                messages.append({"role": "user", "content": text_content})

        # æ™ºèƒ½è·¯ç”±
        has_images = bool(image_data_list)
        target_model, thinking_level, need_search = await self._route_model(content, has_images)

        print(f"ğŸ¯ [AIHandler] è·¯ç”±ç»“æœ: model={target_model}, thinking={thinking_level}, search={need_search}")

        # è°ƒç”¨ AI æµå¼æ¥å£
        full_response = ""
        full_thinking = ""
        usage_info = None

        try:
            # æ ¹æ®åç«¯é€‰æ‹©è°ƒç”¨ä¸åŒçš„ API
            if AI_BACKEND == "openclaw":
                from app.openclaw_client import call_openclaw_stream
                stream = call_openclaw_stream(
                    messages,
                    conversation_id=session_key,
                    sender_id=user_id,
                    sender_nick=sender_nick,
                    model=target_model
                )
            else:
                stream = call_gemini_stream(
                    messages,
                    target_model=target_model,
                    thinking_level=thinking_level,
                    enable_search=need_search
                )

            async for chunk in stream:
                # å¤„ç†ä½¿ç”¨ç»Ÿè®¡
                if "usage" in chunk:
                    usage_info = chunk["usage"]
                    continue

                if "error" in chunk:
                    error_msg = chunk["error"]
                    print(f"âŒ AI è¯·æ±‚å¤±è´¥: {error_msg}")
                    return f"âŒ **API è¯·æ±‚å¤±è´¥**\n\n{error_msg}"

                # å¤„ç† thinking
                thinking_delta = chunk.get("thinking", "")
                if thinking_delta:
                    full_thinking += thinking_delta
                    if stream_callback:
                        await stream_callback(thinking=full_thinking, content=full_response, is_thinking=True)
                    continue

                # å¤„ç†æ­£å¼å›å¤
                content_delta = chunk.get("content", "")
                if content_delta:
                    content_delta = content_delta.replace("[AILoading]", "")
                    full_response += content_delta
                    if stream_callback:
                        await stream_callback(thinking=full_thinking, content=full_response, is_thinking=False)

            print(f"âœ… [AIHandler] æµå¼å“åº”ç»“æŸï¼Œæ€»é•¿åº¦: {len(full_response)}, thinking: {len(full_thinking)}")

            # æ¸…ç†å›å¤
            full_response = full_response.replace("[AILoading]", "").strip()

            # è®°å½•å†å²
            update_history(session_key, user_msg=None, assistant_msg=full_response)

            # è°ƒç”¨å®Œæˆå›è°ƒ
            if complete_callback:
                await complete_callback(full_response, full_thinking, usage_info)

            return full_response

        except Exception as e:
            error_msg = f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}"
            print(f"ğŸ’¥ [AIHandler] {error_msg}")
            import traceback
            traceback.print_exc()
            return f"ğŸ’¥ **ç³»ç»Ÿå¼‚å¸¸**\n\n{error_msg}"

    def _build_system_prompt(self, group_info: Optional[Dict] = None) -> str:
        """æ„å»º System Prompt"""
        beijing_tz = timezone(timedelta(hours=8))
        current_time = datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S")
        current_date = datetime.now(beijing_tz)
        year = current_date.year
        month = current_date.month
        day = current_date.day

        # æ ¹æ® AI_BACKEND åŠ¨æ€è®¾ç½® bot åç§°
        bot_name = {"gemini": "Gem", "openclaw": "Claw"}.get(AI_BACKEND, "Gem")

        system_prompt = f"""ä½ æ˜¯ {bot_name}ï¼Œä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ã€‚ä½ çš„å›ç­”åº”è¯¥å‡†ç¡®ï¼Œä¸è¦äº§ç”Ÿå¹»è§‰ã€‚

â° é‡è¦æ—¶é—´ä¿¡æ¯ï¼ˆè¯·åŠ¡å¿…è®°ä½ï¼‰:
- ä»Šå¤©æ˜¯: {year} å¹´ {month} æœˆ {day} æ—¥
- å½“å‰å®Œæ•´æ—¶é—´: {current_time} (åŒ—äº¬æ—¶é—´, UTC+8)
- ä½ çš„è®­ç»ƒæ•°æ®å¯èƒ½æˆªæ­¢äº 2025 å¹´ï¼Œä½†ç°åœ¨å·²ç»æ˜¯ {year} å¹´äº†
- å½“å›ç­”æ¶‰åŠ"ä»Šå¹´"ã€"ç°åœ¨"ã€"å½“å‰"ç­‰æ—¶é—´ç›¸å…³é—®é¢˜æ—¶ï¼Œè¯·ä½¿ç”¨ä¸Šè¿°æ—¥æœŸè€Œéè®­ç»ƒæ•°æ®ä¸­çš„æ—¶é—´

æ ¼å¼è§„åˆ™:
1. ä¸è¦ä½¿ç”¨ LaTeX è¯­æ³•ï¼ˆå¦‚ $x^2$ æˆ– $$...$$ï¼‰ã€‚ç”¨çº¯æ–‡æœ¬æˆ– Unicode è¡¨ç¤ºæ•°å­¦å…¬å¼ï¼ˆå¦‚ x^2, sqrt(x)ï¼‰ã€‚
2. å¯ä»¥ä½¿ç”¨ Markdownï¼šè¡¨æ ¼ã€åŠ ç²—ã€æ–œä½“ã€åˆ—è¡¨ã€ä»£ç å—ã€‚

ä¸Šä¸‹æ–‡æ„ŸçŸ¥:
- å¯¹è¯å†å²ä¸­åŒ…å«ç”¨æˆ·æ˜µç§°å’Œæ—¶é—´æˆ³ï¼Œæ ¼å¼ä¸º '[æ—¶é—´] æ˜µç§°: æ¶ˆæ¯'ã€‚
- å¼•ç”¨ç”¨æˆ·å‘è¨€æ—¶ï¼Œå¯ä»¥æåŠå…¶æ˜µç§°å’Œæ—¶é—´ï¼ˆå¦‚ 'æ­£å¦‚å¼ ä¸‰åœ¨ 14:30 æ‰€è¯´...'ï¼‰ã€‚
- æ‰€æœ‰æ—¶é—´å‡ä¸ºåŒ—äº¬æ—¶é—´ (UTC+8)ã€‚
- AI å›å¤å¯èƒ½å¸¦æœ‰æ¥æºæ ‡ç­¾ [Gem] æˆ– [Claw]ï¼Œè¡¨ç¤ºç”±ä¸åŒ AI åŠ©æ‰‹ç”Ÿæˆã€‚
- ä½ æ˜¯ {bot_name}ï¼Œå›å¤ä¸éœ€è¦æ·»åŠ æ¥æºæ ‡ç­¾ã€‚

é‡ç‚¹:
- ç›´æ¥å›åº”æœ€æ–°ç”¨æˆ·çš„è¾“å…¥ã€‚
- ä»…å°†ä¹‹å‰çš„ä¸Šä¸‹æ–‡ä½œä¸ºå‚è€ƒã€‚

è¾“å‡ºè¦æ±‚:
- ç›´æ¥è¾“å‡ºç­”æ¡ˆã€‚ä¸è¦è¾“å‡ºçŠ¶æ€æŒ‡ç¤ºå™¨æˆ– '[AILoading]'ã€‚
- ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚æŠ€æœ¯æœ¯è¯­å¯åœ¨ä¸­æ–‡ååŠ è‹±æ–‡æ‹¬å·ï¼ˆå¦‚ï¼šæœºå™¨å­¦ä¹  (Machine Learning)ï¼‰ã€‚

æœç´¢å’Œå®æ—¶ä¿¡æ¯:
- å¦‚æœå¯ç”¨äº† Google Searchï¼Œæœç´¢ç»“æœä¼šè‡ªåŠ¨æä¾›ç»™ä½ 
- å½“æœç´¢ç»“æœä¸ä½ çš„è®­ç»ƒæ•°æ®å†²çªæ—¶ï¼Œä¼˜å…ˆç›¸ä¿¡æœç´¢ç»“æœ
- ç‰¹åˆ«æ˜¯æ¶‰åŠæ—¶é—´ã€æ—¥æœŸã€æœ€æ–°äº‹ä»¶æ—¶ï¼Œæœç´¢ç»“æœæ¯”è®­ç»ƒæ•°æ®æ›´å‡†ç¡®
- å¦‚æœç”¨æˆ·è´¨ç–‘ä½ å¯¹æ—¶é—´çš„è®¤çŸ¥ï¼Œè¯·å†æ¬¡ç¡®è®¤ï¼šä»Šå¤©æ˜¯ {year} å¹´ {month} æœˆ {day} æ—¥

åœ°ç†å’Œæ—¶åŒºè§„åˆ™:
- é»˜è®¤æŒ‰åŒ—äº¬æ—¶é—´ (Asia/Shanghai, UTC+8) å›ç­”æ—¶é—´ç›¸å…³é—®é¢˜ã€‚
- ç”¨æˆ·æœªæ˜ç¡®ç»™å‡ºåŸå¸‚æ—¶ï¼Œé»˜è®¤æŒ‰ä¸­å›½å¤§é™†åœºæ™¯ç†è§£ï¼Œå¹¶ä¼˜å…ˆè¿½é—®å…·ä½“åŸå¸‚ã€‚
- ä¸è¦ä»…ä¾æ® IP/ä»£ç†/VPN æ¨æ–­ç”¨æˆ·åœ¨æµ·å¤–ï¼›è‹¥å®šä½å†²çªï¼Œä»¥ç”¨æˆ·æ˜ç¡®åœ°ç‚¹ä¸ºå‡†ã€‚

æ€è€ƒè¯­è¨€:
- è¯·ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ€è€ƒå’Œæ¨ç†ã€‚ä½ çš„å†…éƒ¨æ€è€ƒè¿‡ç¨‹ä¹Ÿåº”è¯¥ç”¨ä¸­æ–‡è¡¨è¾¾ã€‚"""

        # æ³¨å…¥ç¾¤ä¿¡æ¯
        if group_info:
            group_name = group_info.get('name', 'Unknown Group')
            group_context = f"\n\nGROUP CONTEXT:\nYou are currently in a group chat named '{group_name}'.\n\nTASK:\nBased on the group name, briefly analyze what technical capabilities or domain knowledge you might need to assist this group effectively. Keep this analysis internal to guide your responses."
            system_prompt += group_context

        return system_prompt

    def _format_history(self, history_messages: List[Dict]) -> List[Dict]:
        """æ ¼å¼åŒ–å†å²æ¶ˆæ¯"""
        formatted_history = []
        for msg in history_messages:
            formatted_msg = {"role": msg["role"]}
            msg_content = msg.get("content", "")
            timestamp = msg.get("timestamp")

            # å¦‚æœæœ‰æ—¶é—´æˆ³ï¼Œæ·»åŠ åˆ°å†…å®¹å‰é¢
            if timestamp and msg["role"] == "user":
                formatted_msg["content"] = f"[{timestamp}] {msg_content}"
            elif msg["role"] == "assistant" and msg.get("bot_id"):
                # assistant æ¶ˆæ¯æœ‰ bot_id æ—¶ï¼ŒåŠ æ¥æºæ ‡ç­¾
                msg_bot_id = msg["bot_id"]
                bot_label = {"gemini": "Gem", "openclaw": "Claw"}.get(msg_bot_id, msg_bot_id)
                formatted_msg["content"] = f"[{bot_label}] {msg_content}"
            else:
                formatted_msg["content"] = msg_content

            formatted_history.append(formatted_msg)

        return formatted_history

    async def _route_model(self, content: str, has_images: bool) -> tuple:
        """
        æ™ºèƒ½è·¯ç”±ï¼šé€‰æ‹©æ¨¡å‹ã€thinking level å’Œæ˜¯å¦è”ç½‘

        Returns:
            (target_model, thinking_level, need_search)
        """
        if AI_BACKEND == "openclaw":
            # OpenClaw æ¨¡å¼: Gateway è‡ªè¡Œå†³å®šæ¨¡å‹å’Œ thinkingï¼Œå®¢æˆ·ç«¯æ— æ³•æ§åˆ¶
            return ("openclaw", "default", False)
        else:
            # Gemini æ¨¡å¼: æ™ºèƒ½è·¯ç”±åˆ†æ
            try:
                complexity = await analyze_complexity_with_model(content, has_images)
                print(f"ğŸ”„ [è·¯ç”±] é¢„åˆ†æè¿”å›: {complexity}")
            except Exception as e:
                print(f"âŒ [è·¯ç”±] é¢„åˆ†æå¼‚å¸¸: {e}")
                complexity = {
                    "model": "gemini-3-flash-preview",
                    "thinking_level": "low",
                    "need_search": False,
                    "reason": "è·¯ç”±å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤"
                }

            target_model = complexity.get("model", "gemini-3-flash-preview")
            thinking_level = complexity.get("thinking_level", "low")
            need_search = complexity.get("need_search", False)

            return (target_model, thinking_level, need_search)
